{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_Autograd_3",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMwuyBDVb7UDgRdBEwDAaPq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ejrtks1020/Pytorch-basic/blob/main/pytorch-tutorials/pytorch_Autograd_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## reference : https://tutorials.pytorch.kr/beginner/basics/autogradqs_tutorial.html"
      ],
      "metadata": {
        "id": "iVgBSyNOLgRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# torch.autograd를 사용한 자동미분\n",
        "\n",
        "역전파 알고리즘에서 weight(매개변수)는 주어진 weight에 대한 손실함수의 gradient에 따라 update됨\n",
        "\n",
        "Pytorch에는 torch.autograd라는 자동 미분엔진이 내장되어 있다."
      ],
      "metadata": {
        "id": "AU0mvWSuL2Sa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MExMpNCl1aaC"
      },
      "outputs": [],
      "source": [
        "# example\n",
        "import torch\n",
        "\n",
        "x = torch.ones(5) # input tensor\n",
        "y = torch.zeros(3) # expected output\n",
        "w = torch.randn(5, 3, requires_grad = True)\n",
        "b = torch.randn(3, requires_grad = True)\n",
        "z = torch.matmul(x, w) + b\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Gradient function for z = ', z.grad_fn)\n",
        "print('Gradient function for loss = ', loss.grad_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tXMP2HKM6t-",
        "outputId": "ad2647d7-8200-441e-f634-17638659d7c6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient function for z =  <AddBackward0 object at 0x7fc1c5159d90>\n",
            "Gradient function for loss =  <BinaryCrossEntropyWithLogitsBackward0 object at 0x7fc1c5159f90>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 변화도(Gradient) 계산하기"
      ],
      "metadata": {
        "id": "yFnQG_QkNdqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 신경망에서 가중치를 최적호 하려면 가중치에 대한 손실함수의 도함수(derivate)계산해야함\n",
        "# 도함수 계산을 위해 loss.backward() 호출\n",
        "loss.backward()\n",
        "\n",
        "# requires_grad = True인 텐서들의 gradient 출력\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFWDLlOHNY5t",
        "outputId": "da4708de-587c-473f-ea4e-d1bc17c37271"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1403, 0.3212, 0.1119],\n",
            "        [0.1403, 0.3212, 0.1119],\n",
            "        [0.1403, 0.3212, 0.1119],\n",
            "        [0.1403, 0.3212, 0.1119],\n",
            "        [0.1403, 0.3212, 0.1119]])\n",
            "tensor([0.1403, 0.3212, 0.1119])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradient 계산 멈추기"
      ],
      "metadata": {
        "id": "3PKU33ckOkFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델을 학습한뒤 입력데이터를 적용하는 순전파 연산만 필요한 경우, torch.no_grad()로 연산계산을 멈춘다.\n",
        "\n",
        "z = torch.matmul(x, w) + b\n",
        "print(z.requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "  z = torch.matmul(x, w) + b\n",
        "print(z.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86Z-6MbzN5w3",
        "outputId": "fbf77cb5-19cd-4561-d5d1-c1c1e04afc2b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient update를 멈추는 또 다른 방법\n",
        "z = torch.matmul(x, w) + b\n",
        "z_det = z.detach()\n",
        "print(z_det.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyNo0QrOO8IC",
        "outputId": "97be0a2f-3d93-47f6-faf8-119f7d8b04ee"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gradient update를 멈춰야 하는 이유\n",
        "* transfer learning(전이학습)시 사전학습된 신경망을 fine-tuning(미세조정)할때, 신경망의 일부 매개변수를 고정된 매개변수로 표시하기때문(frozen parameter)\n",
        "\n",
        "* gradient를 추적하지 않는 텐서의 연산이 더 효율적이기 때문에, 순저파 단계만 수행할 때 연산 속도가 향상됨"
      ],
      "metadata": {
        "id": "OGUZbhrsPO7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_uJXXJTuPLys"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}